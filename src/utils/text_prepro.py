import re
import collections
import torch
import unicodedata
import os
import random
## ì „ì²˜ë¦¬ ë©”ì„œë“œ

def prepare_eng_fra_splits(
    file_path: str,
    outdir: str | None = None,
    train_ratio: float = 0.8,
    dev_ratio: float = 0.1,
    test_ratio: float = 0.1,
    min_len: int = 1,
    max_len: int = 60,
    lower: bool = True,        # clean_str(..., True)ì™€ ì¼ì¹˜
    dedupe: bool = True,
    seed: int = 54321,
    save_format: str = "paired",   # "paired" => .tsv  /  "parallel" => .en/.fr
):
    """
    eng-fra 'í•œ íŒŒì¼(ì˜ë¬¸\\të¶ˆë¬¸)'ì„ ì½ì–´ ì „ì²˜ë¦¬/ë¶„í• í•˜ê³ ,
    (ì„ íƒ) ë””ìŠ¤í¬ì— ì €ì¥ê¹Œì§€ í•˜ëŠ” 'í•œ ë°©' ìœ í‹¸.

    return:
      {
        "train": (src_list, tgt_list),
        "dev":   (src_list, tgt_list),
        "test":  (src_list, tgt_list),
        "paths": { ì €ì¥ ê²½ë¡œë“¤ (outdir ì§€ì •ì‹œ) }
      }
    """
    assert abs(train_ratio + dev_ratio + test_ratio - 1.0) < 1e-6, "split ratio í•©ì€ 1ì´ì–´ì•¼ í•¨"
    assert save_format in {"paired", "parallel"}

    # 1) íŒŒì¼ ë¡œë“œ (ë„¤ ê¸°ì¡´ clean_str ë¡œì§ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
    src_all, tgt_all = [], []
    with open(file_path, encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line or "\t" not in line:
                continue
            s, t = line.split("\t", 1)
            s = clean_str(s, nmt=lower)  # lower=Trueë©´ unicode normalize+ì†Œë¬¸ì
            t = clean_str(t, nmt=lower)
            # ê¸¸ì´ í•„í„° (ì•„ì£¼ ë‹¨ìˆœí•œ ê³µë°± ë¶„í•  ê¸°ì¤€)
            sl, tl = len(s.split()), len(t.split())
            if (sl < min_len or tl < min_len) or (sl > max_len or tl > max_len):
                continue
            src_all.append(s)
            tgt_all.append(t)

    # 2) (ì„ íƒ) ì¤‘ë³µ ì œê±°
    if dedupe:
        seen = set()
        new_src, new_tgt = [], []
        for s, t in zip(src_all, tgt_all):
            key = (s, t)
            if key in seen:
                continue
            seen.add(key)
            new_src.append(s)
            new_tgt.append(t)
        src_all, tgt_all = new_src, new_tgt

    # 3) ì…”í”Œ + ë¶„í•  (ì¬í˜„ì„± ë³´ì¥)
    pairs = list(zip(src_all, tgt_all))
    rnd = random.Random(seed)
    rnd.shuffle(pairs)
    n = len(pairs)
    n_train = int(n * train_ratio)
    n_dev   = int(n * dev_ratio)
    train_pairs = pairs[:n_train]
    dev_pairs   = pairs[n_train:n_train + n_dev]
    test_pairs  = pairs[n_train + n_dev:]

    def unzip(ps):
        if not ps:
            return [], []
        a, b = zip(*ps)
        return list(a), list(b)

    train_src, train_tgt = unzip(train_pairs)
    dev_src,   dev_tgt   = unzip(dev_pairs)
    test_src,  test_tgt  = unzip(test_pairs)

    result = {
        "train": (train_src, train_tgt),
        "dev":   (dev_src, dev_tgt),
        "test":  (test_src, test_tgt),
        "paths": {}
    }

    # 4) (ì„ íƒ) ì €ì¥
    if outdir:
        os.makedirs(outdir, exist_ok=True)
        if save_format == "paired":
            def dump_tsv(path, src, tgt):
                with open(path, "w", encoding="utf-8") as f:
                    for s, t in zip(src, tgt):
                        f.write(f"{s}\t{t}\n")

            p_train = os.path.join(outdir, "train.en-fr.tsv")
            p_dev   = os.path.join(outdir, "dev.en-fr.tsv")
            p_test  = os.path.join(outdir, "test.en-fr.tsv")
            dump_tsv(p_train, train_src, train_tgt)
            dump_tsv(p_dev,   dev_src,   dev_tgt)
            dump_tsv(p_test,  test_src,  test_tgt)
            result["paths"] = {"train": p_train, "dev": p_dev, "test": p_test}

        else:  # parallel
            def dump_parallel(prefix, src, tgt):
                with open(prefix + ".en", "w", encoding="utf-8") as fe, \
                     open(prefix + ".fr", "w", encoding="utf-8") as ff:
                    for s in src: fe.write(s + "\n")
                    for t in tgt: ff.write(t + "\n")

            p_train = os.path.join(outdir, "train")
            p_dev   = os.path.join(outdir, "dev")
            p_test  = os.path.join(outdir, "test")
            dump_parallel(p_train, train_src, train_tgt)
            dump_parallel(p_dev,   dev_src,   dev_tgt)
            dump_parallel(p_test,  test_src,  test_tgt)
            result["paths"] = {
                "train": {"src": p_train + ".en", "tgt": p_train + ".fr"},
                "dev":   {"src": p_dev   + ".en", "tgt": p_dev   + ".fr"},
                "test":  {"src": p_test  + ".en", "tgt": p_test  + ".fr"},
            }

    return result


### 


def unicodeToAscii(s):
    return ''.join(
        c for c in unicodedata.normalize('NFD', s)
        if unicodedata.category(c) != 'Mn')

def load_nmt_data(src_path, tgt_path):
    # ê° ì¤„: ì„œë¡œ ê°™ì€ ì¸ë±ìŠ¤ë¼ë¦¬ í•œ ìŒ
    with open(src_path, encoding='utf-8') as fsrc, open(tgt_path, encoding='utf-8') as ftgt:
        src_lines = [clean_str(l.strip(), True) for l in fsrc if l.strip()]
        tgt_lines = [clean_str(l.strip(), True) for l in ftgt if l.strip()]

    # ê¸¸ì´ ë§ì¶”ê¸°(í˜¹ì‹œ ê³µë°± ì¤„ì´ ë‹¬ë¼ì¡Œì„ ë•Œ ë°©ì–´)
    n = min(len(src_lines), len(tgt_lines))
    src_lines, tgt_lines = src_lines[:n], tgt_lines[:n]
    return src_lines, tgt_lines

def load_nmt_pair_data(file_path):
    print("Reading lines...")
    # íŒŒì¼ ì½ê³  ì¤„ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê¸°
    lines = open(file_path, encoding='utf-8').read().strip().split('\n')
    #  ê° ì¤„ì„ pairs ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê³  normalize í•¨ìˆ˜ ì§„í–‰
    source_text = []
    target_text = []
    for l in lines:
        s, t = l.split('\t')
        source_text.append(clean_str(s, True))
        target_text.append(clean_str(t, True))

    return source_text, target_text
# Source,target  -> [ ]source_text = ["i am fine .", "good morning"]
## target_text = ["je vais bien .", "bonjour"]

def clean_str(string, nmt=False):
    string = re.sub(r"[^A-Za-z0-9(),!?\'\`]", " ", string)
    string = re.sub(r"\'s", " \'s", string)
    string = re.sub(r"\'ve", " \'ve", string)
    string = re.sub(r"n\'t", " n\'t", string)
    string = re.sub(r"\'re", " \'re", string)
    string = re.sub(r"\'d", " \'d", string)
    string = re.sub(r"\'ll", " \'ll", string)
    string = re.sub(r",", " , ", string)
    string = re.sub(r"!", " ! ", string)
    string = re.sub(r"\(", " \( ", string)
    string = re.sub(r"\)", " \) ", string)
    string = re.sub(r"\?", " \? ", string)
    string = re.sub(r"\s{2,}", " ", string)
    if nmt:
        return unicodeToAscii(string.strip().lower())
    else:
        return string.strip().lower()

def load_snips_data(file_path, label_dictionary):
    # Load data from files
    text = list(open(file_path+"/seq.in", "r", encoding='UTF-8').readlines())
    text = [clean_str(sent) for sent in text]
    labels_text = list(open(file_path+"/label", "r", encoding='UTF-8').readlines())
    labels_text = [label.strip() for label in labels_text]

    if len(label_dictionary) == 0:
        label_set = set(labels_text)
        for i, label in enumerate(label_set):
            label_dictionary[label] = i
    labels = [label_dictionary[label_text] for label_text in labels_text]
    return text, labels, label_dictionary

def load_mr_data(pos_file, neg_file):
    pos_text = list(open(pos_file, "r", encoding='latin-1').readlines()) # ê¸ì •ì ì¸ review ì½ì–´ì„œ list í˜•íƒœë¡œ ê´€ë¦¬
    pos_text = [clean_str(sent) for sent in pos_text] # clean_str í•¨ìˆ˜ë¡œ ì „ì²˜ë¦¬ (ì†Œë¬¸ì, íŠ¹ìˆ˜ ê¸°í˜¸ ì œê±°, (), ë“± ë¶„ë¦¬)

    neg_text = list(open(neg_file, "r", encoding='latin-1').readlines()) # ë¶€ì •ì ì¸ review ì½ì–´ì„œ list í˜•íƒœë¡œ ê´€ë¦¬
    neg_text = [clean_str(sent) for sent in neg_text]

    positive_labels = [1 for _ in pos_text] # ê¸ì • review ê°œìˆ˜ë§Œí¼ ground_truth ìƒì„±
    negative_labels = [0 for _ in neg_text] # ë¶€ì • review ê°œìˆ˜ë§Œí¼ ground_truth ìƒì„±
    y = positive_labels + negative_labels

    x_final = pos_text + neg_text
    return [x_final, y]

def buildVocab(sentences, vocab_size):
    ##: ["i am a boy", "you are a girl"] =sentence í˜„ì¬ ì´ë ‡ê²Œ ë˜ì–´ìˆìŒ. 
    # Build vocabulary
    words = []
    for sentence in sentences:
        words.extend(sentence.split()) # ["i","am","a","boy","you","are","a","girl"]
    print("The number of words: ", len(words))
    word_counts = collections.Counter(words) ##	â€¢	ë‹¨ì–´ë³„ ë“±ì¥ íšŸìˆ˜ë¥¼ ì„¸ëŠ” í•´ì‹œ ë§µ. ì˜ˆ: {"a":2, "i":1, "am":1, "boy":1, "you":1, "are":1, "girl":1}
    # Mapping from index to word
    vocabulary_inv = [x[0] for x in word_counts.most_common(vocab_size)] 
    ##	â€¢	[x[0] for ...]ë¡œ ë‹¨ì–´ë§Œ ë½‘ì•„ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“¦ â†’ â€œIDâ†’ë‹¨ì–´â€ ë¦¬ìŠ¤íŠ¸ì˜ ì´ˆì•ˆ. [a,b,c,d, ë¹ˆë„ìˆœ ì •ë ¬ë¦¬ìŠ¤íŠ¸]
    
    # vocabulary_inv = list(sorted(vocabulary_inv)) >ë§Œì•½ ì´ ì¤„ì„ ì¼œë©´ â€œë¹ˆë„ìˆœâ€ì´ ì•„ë‹ˆë¼ â€œì‚¬ì „ìˆœâ€ìœ¼ë¡œ ë°”ë€Œë‹ˆ, ì¼ë°˜ì ìœ¼ë¡œëŠ” ê·¸ëŒ€ë¡œ ë‘ëŠ” ê²Œ ë§ìŒ(ë¹ˆë„ìˆœ ìœ ì§€).
    # Mapping from word to index
    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)} # a: 0, i: 1...
    
    ## enumerate ì˜ˆì‹œ
    ## fruits = ["apple", "banana", "cherry"]

    # for i, fruit in enumerate(fruits):
    #     print(i, fruit)  -> 0 apple 1 banana 2 cherry 
    # // enumerate -> ì¸ë±ìŠ¤ ,ìš”ì†Œì˜ íŠœí”ŒìŒë“¤ì„ ë¦¬ìŠ¤íŠ¸ì— ë‹´ì•„ì¤Œ. ê·¸ë˜ì„œ ìœ„ì—ì„œëŠ”x:i ë¥¼ ë°˜ì „ì‹œí‚´.[(0, "apple"), (1, "banana"), (2, "cherry")] ì„ë§Œë“¤ì–´ëƒ„
    
    
    
    return [vocabulary, vocabulary_inv]
            #     ##
            #     vocabulary_inv=["a","i","am","boy","you","are","girl"] â†’
            # {"a":0,"i":1,"am":2,"boy":3,"you":4,"are":5,"girl":6} ë‹¨ì–´â†’IDâ€ ë§¤í•‘ 
            
def text_to_indices(x_text, word_id_dict, use_unk=False):
    text_indices = []

    for text in x_text:
        words = text.split()
        ids = [2]  # <s>
        for word in words: # i, am, a, boy
            if word in word_id_dict:
                word_id = word_id_dict[word]
            else:  # oov
                if use_unk:
                    word_id = 1 # OOV (out-of-vocabulary)
                else:
                    word_id = len(word_id_dict)
                    word_id_dict[word] = word_id
            ids.append(word_id) # 5, 8, 6, 19
        ids.append(3)  # </s>
        text_indices.append(ids)
    return text_indices
        # #x_text = ["i love you"]
        # word_id_dict = {"<pad>":0, "<unk>":1, "<s>":2, "</s>":3, "i":4, "love":5, "you":6}
        #[[2, 4, 5, 6, 3]]
        ### x_text = ["i hate you"]
        #word_id_dict = {"<pad>":0, "<unk>":1, "<s>":2, "</s>":3, "i":4, "you":6}
        #[[2, 4, 1, 6, 3]] ì´ê²Œ ë§Œì•½ì— unk false í•´ë†“ìœ¼ë©´ ê·¸ëƒ¥   word_id_dictì— ìƒˆë¡œìš´ë‹¨ì–´ ì¶”ê°€ë¨
        #ğŸš« Dev/Testì—ì„œ vocabì„ ë°”ê¾¸ë©´ ì•ˆ ë˜ëŠ” ì´ìœ 
        # 	1.	ëª¨ë¸ íŒŒë¼ë¯¸í„°(ì„ë² ë”© í¬ê¸°) ê³ ì • ë¬¸ì œ
            # 	â€¢	ëª¨ë¸ì€ vocab_size Ã— embedding_dim í¬ê¸°ì˜ ì„ë² ë”© í–‰ë ¬ì„ í•™ìŠµí•¨.
            # 	â€¢	vocabì— ìƒˆë¡œìš´ ë‹¨ì–´ë¥¼ ì¶”ê°€í•˜ë©´ vocab_sizeê°€ ë‹¬ë¼ì ¸ì„œ, í–‰ë ¬ ì°¨ì›ì´ ë°”ë€Œì–´ ë²„ë¦¼.
            # 	â€¢	ì´ë¯¸ í•™ìŠµëœ íŒŒë¼ë¯¸í„°ì™€ ë§ì§€ ì•Šì•„ì„œ ë¶ˆì¼ì¹˜ ë°œìƒ â†’ ëª¨ë¸ ì‹¤í–‰ ë¶ˆê°€.
            # 	2.	í‰ê°€ì˜ ê³µì •ì„±/ì¬í˜„ì„±
            # 	â€¢	dev/testëŠ” â€œëª¨ë¸ì´ ë³¸ ì  ì—†ëŠ” ë°ì´í„°â€ë¥¼ í‰ê°€í•˜ëŠ” ìš©ë„ì•¼.
            # 	â€¢	ë§Œì•½ dev/testì—ì„œ ìƒˆ ë‹¨ì–´ë¥¼ ì¶”ê°€í•˜ë©´, ì‚¬ì‹¤ìƒ ëª¨ë¸ì´ í‰ê°€ ë‹¨ê³„ì—ì„œ ìƒˆ ì§€ì‹ì„ ì–»ê²Œ ë˜ëŠ” ê¼´ì´ ë¼.
            # â†’ í‰ê°€ê°€ ë¶ˆê³µì •í•˜ê³  ê²°ê³¼ ì¬í˜„ë„ ì•ˆ ë¨.
            # 	3.	OOV ì²˜ë¦¬ëŠ” ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ í™•ì¸ìš©
            # 	â€¢	í˜„ì‹¤ ì„¸ê³„ ë°ì´í„°ì—ëŠ” í•­ìƒ trainì— ì—†ë˜ ë‹¨ì–´ê°€ ë‚˜íƒ€ë‚¨.
            # 	â€¢	ì´ê±¸ <unk>ë¡œ ë¬¶ì–´ì•¼ ëª¨ë¸ì´ â€œë‚¯ì„  ë‹¨ì–´ë„ ë¬¸ë§¥ìœ¼ë¡œ ëŒ€ì¶© ë²ˆì—­í•œë‹¤â€ëŠ” ì¼ë°˜í™” ëŠ¥ë ¥ì„ ë³´ì¼ ìˆ˜ ìˆì–´.
            # 	â€¢	ë§Œì•½ ìƒˆ ë‹¨ì–´ë¥¼ ì¶”ê°€í•´ë²„ë¦¬ë©´, dev/test ì„±ëŠ¥ì´ ì‹¤ì œë³´ë‹¤ ë” ì¢‹ê²Œ ë‚˜ì™€ì„œ ì°©ì‹œ íš¨ê³¼ê°€ ìƒê¹€.
        ##ex
            #         #2 â†’ [0.2, -0.1, 0.05, ...]   # <s> ë²¡í„°
            # 4 â†’ [0.3, 0.9, -0.7, ...]    # "i"
            # 5 â†’ [0.8, -0.2, 0.1, ...]    # "love"
            # 6 â†’ [-0.4, 0.3, 0.6, ...]    # "you"
            # 3 â†’ [0.0, 0.0, 0.0, ...]     # </s>
            ## OOV (Out-Of-Vocabulary)
	# â€¢	ë§ ê·¸ëŒ€ë¡œ ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´.
	# â€¢	ìš°ë¦¬ê°€ í•™ìŠµìš© ë‹¨ì–´ ì‚¬ì „(vocabulary) ì„ ë§Œë“¤ ë•Œ Train ë°ì´í„°ì— ë“±ì¥í•œ ë‹¨ì–´ë§Œ ëª¨ì•„ì„œ dictionary(word2id)ë¥¼ ë§Œë“¤ì–´.
	# â€¢	ê·¸ëŸ°ë° Dev/Test ë°ì´í„°ì—ì„œëŠ” í›ˆë ¨ ë•Œ ë³¸ ì  ì—†ëŠ” ìƒˆë¡œìš´ ë‹¨ì–´ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆìŒ â†’ ì´ê±¸ OOVë¼ê³  í•´.

def sequence_to_tensor(sequence_list, nb_paddings=(0, 0)):
    nb_front_pad, nb_back_pad = nb_paddings

    max_length = len(max(sequence_list, key=len)) + nb_front_pad + nb_back_pad
    sequence_tensor = torch.LongTensor(len(sequence_list), max_length).zero_()  # 0: <pad>
    print("\n max length: " + str(max_length))
    for i, sequence in enumerate(sequence_list):
        sequence_tensor[i, nb_front_pad:len(sequence) + nb_front_pad] = torch.tensor(sequence)
    return sequence_tensor